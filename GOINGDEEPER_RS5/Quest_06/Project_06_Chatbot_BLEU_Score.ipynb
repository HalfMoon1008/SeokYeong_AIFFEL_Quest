{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M516hbThK1SM"
   },
   "source": [
    "# **18. 번역가는 대화에도 능하다 [프로젝트]**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_YUYyQ1K9M0"
   },
   "source": [
    "# **18-1. Project: 멋진 챗봇 만들기**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSr3Vb0oK9TL"
   },
   "source": [
    "### **라이브러리 버전을 확인해 봅니다**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Rp8o1mO9K0cn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow  as tf\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QZx7lLJWLM1l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21.4\n",
      "1.3.3\n",
      "2.6.0\n",
      "3.6.5\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(tf.__version__)\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivR0F3LcK9ar"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIo4NRi_SJH9"
   },
   "source": [
    "### **Step 1. 데이터 가져오기**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5BdEP6SUXhUm"
   },
   "outputs": [],
   "source": [
    "# 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgkg6Z20T0j2",
    "outputId": "1d130ae9-bc6c-48cd-cef9-3fb82b499a6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatbotData.csv  ko.bin\r\n"
     ]
    }
   ],
   "source": [
    "!ls /aiffel/aiffel/AIFFEL/09_GD_NLP/Quest_06/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "rdPLo3vgTXQJ",
    "outputId": "e45530fa-e1bd-472f-cb20-4ce7cf94290d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/aiffel/aiffel/AIFFEL/09_GD_NLP/Quest_06/data/ChatbotData.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhv9pdeGV5We"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fkgYE3UNVkTZ"
   },
   "outputs": [],
   "source": [
    "# 데이터의 질문과 답변을 각각 questions, answers 변수에 나눠서 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HvVI7WjDX2wH"
   },
   "outputs": [],
   "source": [
    "questions = df['Q'].tolist()\n",
    "answers = df['A'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbDnYVhWpCG6",
    "outputId": "e9c21020-f321-42bb-d6b3-9e7876d49336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11823\n",
      "11823\n"
     ]
    }
   ],
   "source": [
    "print(len(questions))\n",
    "print(len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mp2vlPYQYOk2",
    "outputId": "c9df9bab-2e48-4fad-f77a-d4c8cf40dc16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12시 땡!', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', 'PPL 심하네']\n",
      "['하루가 또 가네요.', '위로해 드립니다.', '여행은 언제나 좋죠.', '여행은 언제나 좋죠.', '눈살이 찌푸려지죠.']\n"
     ]
    }
   ],
   "source": [
    "print(questions[:5])\n",
    "print(answers[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXVwm2C-YehW"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2lJEs3-Yekl"
   },
   "source": [
    "### **Step 2. 데이터 정제**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SFVtaBfHYR0V"
   },
   "outputs": [],
   "source": [
    "# 아래 조건을 만족하는 preprocess_sentence() 함수 구현\n",
    "# 1. 영문자의 경우, 모두 소문자로 변환\n",
    "# 2. 영문자와 한글, 숫자, 그리고 주요 특수문자를 제외하곤 정규식을 활용하여 모두 제거\n",
    "# 문장부호 양옆에 공백을 추가하는 등 이전과 다르게 생략된 기능들은 우리가 사용할 토크나이저가 지원하기 때문에 굳이 구현하지 않아도 괜찮습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hmnR4etrakzj"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    # 모든 영문자를 소문자로 변환합니다.\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # 영문자, 한글, 숫자, 그리고 주요 특수문자를 제외한 모든 문자를 제거합니다.\n",
    "    # 아래 정규식은 영문자(a-z), 한글(가-힣), 숫자(0-9), 그리고 주요 특수문자를 제외한 나머지 문자를 찾아냅니다.\n",
    "    sentence = re.sub(r'[^a-z가-힣0-9,.?! ]', '', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nP7ZQNd8bUEW"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQEoBkQobUJO"
   },
   "source": [
    "### **Step 3. 데이터 토큰화**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aF5DhaHBa_P2"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "def build_corpus(src_data, tgt_data, tokenizer, max_len=40):\n",
    "    # 토큰화\n",
    "    src_corpus = []\n",
    "    tgt_corpus = []\n",
    "\n",
    "    # 중복 체크를 위한 set\n",
    "    pair_set = set()\n",
    "\n",
    "    for src, tgt in zip(src_data, tgt_data):\n",
    "        # 문장 정제\n",
    "        src_clean = preprocess_sentence(src)\n",
    "        tgt_clean = preprocess_sentence(tgt)\n",
    "\n",
    "        # 토큰화\n",
    "        src_tokens = tokenizer(src_clean)\n",
    "        tgt_tokens = tokenizer(tgt_clean)\n",
    "\n",
    "        # 토큰의 개수가 max_len 이하인지 확인\n",
    "        if len(src_tokens) <= max_len and len(tgt_tokens) <= max_len:\n",
    "            # 중복 문장 확인\n",
    "            src_str = ' '.join(src_tokens)\n",
    "            tgt_str = ' '.join(tgt_tokens)\n",
    "\n",
    "            pair_str = src_str + \" <SEP> \" + tgt_str\n",
    "\n",
    "            if pair_str not in pair_set:\n",
    "                src_corpus.append(src_tokens)\n",
    "                tgt_corpus.append(tgt_tokens)\n",
    "                pair_set.add(pair_str)\n",
    "\n",
    "    return src_corpus, tgt_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgZHMZA93WZ7"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 961
    },
    "id": "Fa8YlzgHdVST",
    "outputId": "d818f6e8-0adc-447b-b65e-c7df00d00212"
   },
   "outputs": [],
   "source": [
    "# MeCab 토크나이저\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pEQRLW0RktNH"
   },
   "outputs": [],
   "source": [
    "# build_corpus 함수를 활용하여 토큰화\n",
    "que_corpus, ans_corpus = build_corpus(questions, answers, mecab.morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "IJEkca80ouGr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11746\n",
      "11746\n"
     ]
    }
   ],
   "source": [
    "print(len(que_corpus))\n",
    "print(len(ans_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "fLprrTPjo1TL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['12', '시', '땡', '!'], ['1', '지망', '학교', '떨어졌', '어'], ['3', '박', '4', '일', '놀', '러', '가', '고', '싶', '다'], ['3', '박', '4', '일', '정도', '놀', '러', '가', '고', '싶', '다'], ['ppl', '심하', '네']]\n",
      "[['하루', '가', '또', '가', '네요', '.'], ['위로', '해', '드립니다', '.'], ['여행', '은', '언제나', '좋', '죠', '.'], ['여행', '은', '언제나', '좋', '죠', '.'], ['눈살', '이', '찌푸려', '지', '죠', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(que_corpus[:5])\n",
    "print(ans_corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSlY3hcFqHgC"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnS_Cf3HqHrw"
   },
   "source": [
    "### **Step 4. Augmentation**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "LK2H9qcBo9Vv"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "w2v = gensim.models.Word2Vec.load(\"/aiffel/aiffel/AIFFEL/09_GD_NLP/Quest_06/data/ko.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "lqvCbDpRsHOo"
   },
   "outputs": [],
   "source": [
    "def lexical_sub(sentence, word2vec):\n",
    "    import random\n",
    "\n",
    "    res = []\n",
    "    toks = sentence\n",
    "\n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "\n",
    "    except:   # 단어장에 없는 단어 or other issues\n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok == _from: \n",
    "            res.append(_to)\n",
    "        else: \n",
    "            res.append(tok)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_corpus(que_corpus, ans_corpus, word2vec):\n",
    "    augmented_que_corpus = []\n",
    "    augmented_ans_corpus = []\n",
    "    \n",
    "    for sentence in que_corpus:\n",
    "        augmented_sentence = lexical_sub(sentence, word2vec)\n",
    "        if augmented_sentence: \n",
    "            augmented_que_corpus.append(augmented_sentence)\n",
    "            augmented_ans_corpus.append(sentence)\n",
    "    \n",
    "    for sentence in ans_corpus:\n",
    "        augmented_sentence = lexical_sub(sentence, word2vec)\n",
    "        if augmented_sentence: \n",
    "            augmented_que_corpus.append(sentence)\n",
    "            augmented_ans_corpus.append(augmented_sentence)\n",
    "            \n",
    "    return augmented_que_corpus, augmented_ans_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_360/650141426.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  _to = word2vec.most_similar(_from)[0][0]\n"
     ]
    }
   ],
   "source": [
    "augmented_que_corpus, augmented_ans_corpus = augment_corpus(que_corpus, ans_corpus, w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20383\n",
      "20383\n"
     ]
    }
   ],
   "source": [
    "print(len(augmented_que_corpus))\n",
    "print(len(augmented_ans_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['12', '시', '땡', '캐치'], ['1', '지망', '학교', '떨어졌', '어서'], ['3', '김', '4', '일', '놀', '러', '가', '고', '싶', '다'], ['3', '김', '4', '일', '정도', '놀', '러', '가', '고', '싶', '다'], ['ppl', '강하', '네']]\n",
      "[['12', '시', '땡', '!'], ['1', '지망', '학교', '떨어졌', '어'], ['3', '박', '4', '일', '놀', '러', '가', '고', '싶', '다'], ['3', '박', '4', '일', '정도', '놀', '러', '가', '고', '싶', '다'], ['ppl', '심하', '네']]\n"
     ]
    }
   ],
   "source": [
    "print(augmented_que_corpus[:5])\n",
    "print(augmented_ans_corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터에 augmented 데이터 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_que_corpus = que_corpus + augmented_que_corpus\n",
    "all_ans_corpus = ans_corpus + augmented_ans_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32129\n",
      "32129\n"
     ]
    }
   ],
   "source": [
    "print(len(all_que_corpus))\n",
    "print(len(all_ans_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['12', '시', '땡', '!'], ['1', '지망', '학교', '떨어졌', '어'], ['3', '박', '4', '일', '놀', '러', '가', '고', '싶', '다'], ['3', '박', '4', '일', '정도', '놀', '러', '가', '고', '싶', '다'], ['ppl', '심하', '네']]\n",
      "[['하루', '가', '또', '가', '네요', '.'], ['위로', '해', '드립니다', '.'], ['여행', '은', '언제나', '좋', '죠', '.'], ['여행', '은', '언제나', '좋', '죠', '.'], ['눈살', '이', '찌푸려', '지', '죠', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(all_que_corpus[:5])\n",
    "print(all_ans_corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5. 데이터 벡터화**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ans_corpus = [[\"<start>\"] + sentence + [\"<end>\"] for sentence in all_ans_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', '하루', '가', '또', '가', '네요', '.', '<end>']\n"
     ]
    }
   ],
   "source": [
    "print(all_ans_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 전체 단어 집합 생성 (all_que_corpus와 all_ans_corpus 결합)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corpus = all_que_corpus + all_ans_corpus\n",
    "all_words = [word for sentence in all_corpus for word in sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 단어와 정수 인덱스 간 매핑 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(all_words)\n",
    "vocab = [\"<pad>\", \"<unk>\"] + [word for word, freq in word_freq.items() if freq > 1]\n",
    "word_to_index = {word: index for index, word in enumerate(vocab)}\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 데이터 벡터화 (각 문장을 해당 정수 인덱스의 시퀀스로 변환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_index(corpus, word_to_index, type):\n",
    "    data = []\n",
    "    for sentence in corpus:\n",
    "        if type == \"dec_input\":  # 디코더 입력일 경우 <start> 토큰을 추가\n",
    "            sentence = [\"<start>\"] + sentence\n",
    "        if type == \"dec_target\":  # 디코더 출력일 경우 <end> 토큰을 추가\n",
    "            sentence = sentence + [\"<end>\"]\n",
    "        data.append([word_to_index.get(word, word_to_index[\"<unk>\"]) for word in sentence])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train = convert_text_to_index(all_que_corpus, word_to_index, \"enc\")\n",
    "dec_input_train = convert_text_to_index(all_ans_corpus, word_to_index, \"dec_input\")\n",
    "dec_target_train = convert_text_to_index(all_ans_corpus, word_to_index, \"dec_target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 시퀀스의 길이를 동일하게 만들고, 데이터를 numpy array로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(s) for s in enc_train)\n",
    "enc_train = pad_sequences(enc_train, maxlen=max_len, padding=\"post\")\n",
    "dec_input_train = pad_sequences(dec_input_train, maxlen=max_len, padding=\"post\")\n",
    "dec_target_train = pad_sequences(dec_target_train, maxlen=max_len, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 원-핫 인코딩 형식으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_target_train = to_categorical(dec_target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "[6872 6872  530   17  318   17 1839   87 6873    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(enc_train[0]) \n",
    "print(dec_input_train[0]) \n",
    "print(dec_target_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32129\n",
      "32129\n",
      "32129\n"
     ]
    }
   ],
   "source": [
    "print(len(enc_train))\n",
    "print(len(dec_input_train))\n",
    "print(len(dec_target_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 6. 훈련하기**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer 모델 구현\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Head Attention 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)\n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position-wise Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        # Q, K, V 순서에 주의하세요!\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "                        for _ in range(n_layers)]\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "\n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "                            for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 전체 모델 조립"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주요 파라미터 설정 및 모델 인스턴스 생성\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "n_layers = 1\n",
    "d_model = 368\n",
    "n_heads = 8\n",
    "d_ff = 1024\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 파라미터 설정\n",
    "warmup_steps = 1000\n",
    "batch_size = 64\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = max(\n",
    "    np.max(enc_train),\n",
    "    np.max(dec_input_train),\n",
    "    np.max(dec_target_train)\n",
    ") + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=n_layers,\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    d_ff=d_ff,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,  \n",
    "    dropout=dropout,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate & Optimizer, Loss Function 정의\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)  # 형변환 추가\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate 인스턴스 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(d_model, warmup_steps=warmup_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    # 여기에서 안전한 로그를 계산\n",
    "    loss_ = tf.where(tf.math.is_nan(loss_), tf.zeros_like(loss_), loss_)\n",
    "    \n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Step 정의\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_step 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]\n",
    "    gold = tgt[:, 1:]\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _, _, _ = model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 5.0)  # 클리핑 추가\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_step 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def val_step(src, tgt, model):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output 비교 대상\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "    predictions, _, _, _ = model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "    loss = loss_function(gold, predictions)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 분리(train / validation)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터의 전체 길이\n",
    "total_len = len(enc_train)\n",
    "\n",
    "# 분할 지점 계산 (90% 지점)\n",
    "split_idx = int(total_len * 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "enc_val = enc_train[split_idx:]\n",
    "enc_train = enc_train[:split_idx]\n",
    "\n",
    "dec_input_val = dec_input_train[split_idx:]\n",
    "dec_input_train = dec_input_train[:split_idx]\n",
    "\n",
    "dec_target_val = dec_target_train[split_idx:]\n",
    "dec_target_train = dec_target_train[:split_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# enc_train, enc_val, \\\n",
    "# dec_input_train, dec_input_val, \\\n",
    "# dec_target_train, dec_target_val = train_test_split(\n",
    "#     enc_train, dec_input_train, dec_target_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def custom_train_test_split(*arrays, test_size=0.1, random_seed=None):\n",
    "#     # Set random seed for reproducibility\n",
    "#     np.random.seed(random_seed)\n",
    "    \n",
    "#     # Check if there's any array to split\n",
    "#     if len(arrays) == 0:\n",
    "#         raise ValueError(\"At least one array required as input\")\n",
    "    \n",
    "#     # Check if all arrays have the same length\n",
    "#     first_array_length = len(arrays[0])\n",
    "#     for array in arrays:\n",
    "#         if len(array) != first_array_length:\n",
    "#             raise ValueError(\"All input arrays must have the same length\")\n",
    "    \n",
    "#     # Calculate indices to split\n",
    "#     indices = np.arange(first_array_length)\n",
    "#     np.random.shuffle(indices)\n",
    "    \n",
    "#     split_idx = int(first_array_length * (1 - test_size))\n",
    "#     train_idx = indices[:split_idx]\n",
    "#     test_idx = indices[split_idx:]\n",
    "    \n",
    "#     # Split the arrays\n",
    "#     result = []\n",
    "#     for array in arrays:\n",
    "#         result.append(array[train_idx])\n",
    "#         result.append(array[test_idx])\n",
    "    \n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simple_train_test_split(*arrays, test_size=0.1, random_seed=None):\n",
    "#     # Set random seed for reproducibility\n",
    "#     np.random.seed(random_seed)\n",
    "    \n",
    "#     # Ensure arrays are not empty and have the same length\n",
    "#     if len(arrays) == 0:\n",
    "#         raise ValueError(\"At least one array required as input\")\n",
    "\n",
    "#     first_array_length = len(arrays[0])\n",
    "#     for array in arrays[1:]:\n",
    "#         if len(array) != first_array_length:\n",
    "#             raise ValueError(\"All input arrays must have the same length\")\n",
    "    \n",
    "#     # Shuffle and split indices\n",
    "#     indices = np.arange(first_array_length)\n",
    "#     np.random.shuffle(indices)\n",
    "    \n",
    "#     split_idx = int(first_array_length * (1 - test_size))\n",
    "    \n",
    "#     # Use list comprehension to split arrays\n",
    "#     return tuple(array[indices[:split_idx]] for array in arrays) + tuple(array[indices[split_idx:]] for array in arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sequential_train_test_split(*arrays, test_size=0.1):\n",
    "#     # Ensure arrays are not empty and have the same length\n",
    "#     if len(arrays) == 0:\n",
    "#         raise ValueError(\"At least one array required as input\")\n",
    "\n",
    "#     first_array_length = len(arrays[0])\n",
    "#     for array in arrays[1:]:\n",
    "#         if len(array) != first_array_length:\n",
    "#             raise ValueError(\"All input arrays must have the same length\")\n",
    "    \n",
    "#     # Calculate split index\n",
    "#     split_idx = int(first_array_length * (1 - test_size))\n",
    "    \n",
    "#     # Split arrays without shuffling\n",
    "#     return tuple(array[:split_idx] for array in arrays) + tuple(array[split_idx:] for array in arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할 비율 설정\n",
    "# train_ratio = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 호출\n",
    "# enc_train, enc_val, dec_input_train, dec_input_val, dec_target_train, dec_target_val = custom_train_test_split(\n",
    "#     enc_train, dec_input_train, dec_target_train, test_size=1-train_ratio\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # 랜덤 시드 설정 (원하시는 값을 사용하실 수 있습니다.)\n",
    "# random_seed = None\n",
    "# np.random.seed(random_seed)\n",
    "\n",
    "# # 첫 번째 배열의 길이 확인\n",
    "# first_array_length = len(enc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분할 비율 설정\n",
    "test_size = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 인덱스 생성 및 셔플\n",
    "# indices = np.arange(first_array_length)\n",
    "# np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분할 지점 계산\n",
    "# split_idx = int(first_array_length * (1 - test_size))\n",
    "# train_idx = indices[:split_idx]\n",
    "# test_idx = indices[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 배열 분할 및 할당\n",
    "# enc_train, enc_val = enc_train[train_idx], enc_train[test_idx]\n",
    "# dec_input_train, dec_input_val = dec_input_train[train_idx], dec_input_train[test_idx]\n",
    "# dec_target_train, dec_target_val = dec_target_train[train_idx], dec_target_train[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터의 전체 길이\n",
    "# first_array_length = len(enc_train)\n",
    "\n",
    "# # 인덱스를 생성하고 섞습니다.\n",
    "# indices = np.arange(first_array_length)\n",
    "# np.random.shuffle(indices)\n",
    "\n",
    "# # 분할 지점을 계산합니다.\n",
    "# split_idx = int(first_array_length * (1 - test_size))\n",
    "\n",
    "# # 인덱스만 저장합니다.\n",
    "# train_indices = indices[:split_idx]\n",
    "# val_indices = indices[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 예시: 배치 데이터를 가져올 때\n",
    "# enc_train = enc_train[train_indices]\n",
    "# dec_input_train = dec_input_train[train_indices]\n",
    "# dec_target_train = dec_target_train[train_indices]\n",
    "\n",
    "# # 검증 데이터를 가져올 때\n",
    "# enc_val = enc_train[val_indices]\n",
    "# dec_input_val = dec_input_train[val_indices]\n",
    "# dec_target_val = dec_target_train[val_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size for enc_train: 28916\n",
      "Validation data size for enc_val: 3213\n",
      "Training data size for dec_input_train: 28916\n",
      "Validation data size for dec_input_val: 3213\n",
      "Training data size for dec_target_train: 28916\n",
      "Validation data size for dec_target_val: 3213\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data size for enc_train:\", len(enc_train))\n",
    "print(\"Validation data size for enc_val:\", len(enc_val))\n",
    "print(\"Training data size for dec_input_train:\", len(dec_input_train))\n",
    "print(\"Validation data size for dec_input_val:\", len(dec_input_val))\n",
    "print(\"Training data size for dec_target_train:\", len(dec_target_train))\n",
    "print(\"Validation data size for dec_target_val:\", len(dec_target_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 훈련시키기\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 451 Loss 2.1294: 100%|██████████| 451/451 [00:33<00:00, 13.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Loss 4.0201 Val Loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 451 Loss 0.9978: 100%|██████████| 451/451 [00:17<00:00, 25.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train Loss 2.5868 Val Loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 451 Loss 0.5832: 100%|██████████| 451/451 [00:17<00:00, 25.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train Loss 1.9425 Val Loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 451 Loss 0.4218: 100%|██████████| 451/451 [00:18<00:00, 24.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train Loss 1.5736 Val Loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Batch 451 Loss 0.3633: 100%|██████████| 451/451 [00:18<00:00, 24.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train Loss 1.3433 Val Loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Batch 451 Loss 0.3022: 100%|██████████| 451/451 [00:18<00:00, 23.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train Loss 1.1769 Val Loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Batch 451 Loss 0.2780: 100%|██████████| 451/451 [00:18<00:00, 24.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Train Loss 1.0536 Val Loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Batch 451 Loss 0.2573: 100%|██████████| 451/451 [00:18<00:00, 24.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Train Loss 0.9466 Val Loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch 451 Loss 0.2435: 100%|██████████| 451/451 [00:18<00:00, 24.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Train Loss 0.8490 Val Loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Batch 451 Loss 0.2268: 100%|██████████| 451/451 [00:18<00:00, 24.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Train Loss 0.7552 Val Loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Batch 451 Loss 0.2186: 100%|██████████| 451/451 [00:18<00:00, 24.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Train Loss 0.6745 Val Loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Batch 451 Loss 0.2004: 100%|██████████| 451/451 [00:18<00:00, 24.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Train Loss 0.6117 Val Loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 Batch 451 Loss 0.1960: 100%|██████████| 451/451 [00:18<00:00, 24.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Train Loss 0.5555 Val Loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 Batch 451 Loss 0.1825: 100%|██████████| 451/451 [00:18<00:00, 24.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Train Loss 0.5022 Val Loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 Batch 451 Loss 0.1857: 100%|██████████| 451/451 [00:18<00:00, 24.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Train Loss 0.4547 Val Loss nan\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 15\n",
    "BATCH_SIZE = 64\n",
    "N_BATCHES = len(enc_train) // BATCH_SIZE\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    tqdm_bar = tqdm(total=N_BATCHES, position=0, leave=True, desc=f\"Training Epoch {epoch+1}\")\n",
    "\n",
    "    # Training\n",
    "    for batch in range(N_BATCHES):\n",
    "        # 데이터 인덱싱 및 배치 처리\n",
    "        start_idx = batch * BATCH_SIZE\n",
    "        end_idx = (batch + 1) * BATCH_SIZE\n",
    "        \n",
    "        src = enc_train[start_idx:end_idx]\n",
    "        dec_input = dec_input_train[start_idx:end_idx]\n",
    "        dec_target = dec_target_train[start_idx:end_idx]\n",
    "\n",
    "        batch_loss = train_step(src, dec_input, transformer, optimizer)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        # tqdm progress bar update\n",
    "        tqdm_bar.set_description(f\"Epoch {epoch + 1} Batch {batch + 1} Loss {batch_loss.numpy():.4f}\")\n",
    "        tqdm_bar.update(1)\n",
    "\n",
    "    tqdm_bar.close()\n",
    "    \n",
    "    # Validation Loss 계산\n",
    "    val_loss = 0\n",
    "    for batch in range(N_BATCHES):\n",
    "        start_idx = batch * BATCH_SIZE\n",
    "        end_idx = (batch + 1) * BATCH_SIZE\n",
    "        \n",
    "        src = enc_val[start_idx:end_idx]\n",
    "        dec_input = dec_input_val[start_idx:end_idx]\n",
    "        dec_target = dec_target_val[start_idx:end_idx]\n",
    "\n",
    "        batch_val_loss = val_step(src, dec_input, transformer)\n",
    "        val_loss += batch_val_loss\n",
    "\n",
    "    avg_train_loss = total_loss / N_BATCHES\n",
    "    avg_val_loss = val_loss / N_BATCHES\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch + 1} Train Loss {avg_train_loss:.4f} Val Loss {avg_val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(np.isnan(enc_train).any())\n",
    "print(np.isnan(enc_val).any())\n",
    "print(np.isnan(dec_input_train).any())\n",
    "print(np.isnan(dec_input_val).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 450, Loss: 0.1856992393732071\n"
     ]
    }
   ],
   "source": [
    "print(f\"Batch {batch}, Loss: {batch_loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_indices = [i for i, s in enumerate(enc_train) if len(s) == 0]\n",
    "if invalid_indices:\n",
    "    print(f\"Found {len(invalid_indices)} invalid samples in enc_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch 0, Loss: 0.5736705660820007\n",
      "Validation Batch 1, Loss: 0.5693771839141846\n",
      "Validation Batch 2, Loss: 0.6407707929611206\n",
      "Validation Batch 3, Loss: 0.7596200108528137\n",
      "Validation Batch 4, Loss: 0.6182177066802979\n",
      "Validation Batch 5, Loss: 0.7638334631919861\n",
      "Validation Batch 6, Loss: 0.5183248519897461\n",
      "Validation Batch 7, Loss: 0.6722216010093689\n",
      "Validation Batch 8, Loss: 0.9318565130233765\n",
      "Validation Batch 9, Loss: 0.9613356590270996\n",
      "Validation Batch 10, Loss: 0.8023685216903687\n",
      "Validation Batch 11, Loss: 0.6581697463989258\n",
      "Validation Batch 12, Loss: 0.6966259479522705\n",
      "Validation Batch 13, Loss: 0.7694821357727051\n",
      "Validation Batch 14, Loss: 0.6459240913391113\n",
      "Validation Batch 15, Loss: 0.8792791366577148\n",
      "Validation Batch 16, Loss: 0.8188066482543945\n",
      "Validation Batch 17, Loss: 0.8949643969535828\n",
      "Validation Batch 18, Loss: 0.767177164554596\n",
      "Validation Batch 19, Loss: 0.8699151277542114\n",
      "Validation Batch 20, Loss: 0.8821015357971191\n",
      "Validation Batch 21, Loss: 0.7162009477615356\n",
      "Validation Batch 22, Loss: 0.8371626138687134\n",
      "Validation Batch 23, Loss: 1.039292812347412\n",
      "Validation Batch 24, Loss: 0.983756959438324\n",
      "Validation Batch 25, Loss: 0.8936085104942322\n",
      "Validation Batch 26, Loss: 0.8868311643600464\n",
      "Validation Batch 27, Loss: 0.7822818160057068\n",
      "Validation Batch 28, Loss: 0.9601253271102905\n",
      "Validation Batch 29, Loss: 0.8323296904563904\n",
      "Validation Batch 30, Loss: 0.8068845272064209\n",
      "Validation Batch 31, Loss: 0.8359208106994629\n",
      "Validation Batch 32, Loss: 0.8569421172142029\n",
      "Validation Batch 33, Loss: 0.7973908185958862\n",
      "Validation Batch 34, Loss: 0.7822046279907227\n",
      "Validation Batch 35, Loss: 0.9666333198547363\n",
      "Validation Batch 36, Loss: 1.173905849456787\n",
      "Validation Batch 37, Loss: 0.7934157252311707\n",
      "Validation Batch 38, Loss: 0.8187505602836609\n",
      "Validation Batch 39, Loss: 0.8455637693405151\n",
      "Validation Batch 40, Loss: 0.8103516101837158\n",
      "Validation Batch 41, Loss: 0.9624497890472412\n",
      "Validation Batch 42, Loss: 0.7259233593940735\n",
      "Validation Batch 43, Loss: 0.8812004327774048\n",
      "Validation Batch 44, Loss: 0.6906077861785889\n",
      "Validation Batch 45, Loss: 0.6849299669265747\n",
      "Validation Batch 46, Loss: 0.8990525603294373\n",
      "Validation Batch 47, Loss: 0.7702791690826416\n",
      "Validation Batch 48, Loss: 0.7820345759391785\n",
      "Validation Batch 49, Loss: 0.9897719025611877\n",
      "Validation Batch 50, Loss: 1.1496857404708862\n",
      "Validation Batch 51, Loss: nan\n",
      "Validation Batch 52, Loss: nan\n",
      "Validation Batch 53, Loss: nan\n",
      "Validation Batch 54, Loss: nan\n",
      "Validation Batch 55, Loss: nan\n",
      "Validation Batch 56, Loss: nan\n",
      "Validation Batch 57, Loss: nan\n",
      "Validation Batch 58, Loss: nan\n",
      "Validation Batch 59, Loss: nan\n",
      "Validation Batch 60, Loss: nan\n",
      "Validation Batch 61, Loss: nan\n",
      "Validation Batch 62, Loss: nan\n",
      "Validation Batch 63, Loss: nan\n",
      "Validation Batch 64, Loss: nan\n",
      "Validation Batch 65, Loss: nan\n",
      "Validation Batch 66, Loss: nan\n",
      "Validation Batch 67, Loss: nan\n",
      "Validation Batch 68, Loss: nan\n",
      "Validation Batch 69, Loss: nan\n",
      "Validation Batch 70, Loss: nan\n",
      "Validation Batch 71, Loss: nan\n",
      "Validation Batch 72, Loss: nan\n",
      "Validation Batch 73, Loss: nan\n",
      "Validation Batch 74, Loss: nan\n",
      "Validation Batch 75, Loss: nan\n",
      "Validation Batch 76, Loss: nan\n",
      "Validation Batch 77, Loss: nan\n",
      "Validation Batch 78, Loss: nan\n",
      "Validation Batch 79, Loss: nan\n",
      "Validation Batch 80, Loss: nan\n",
      "Validation Batch 81, Loss: nan\n",
      "Validation Batch 82, Loss: nan\n",
      "Validation Batch 83, Loss: nan\n",
      "Validation Batch 84, Loss: nan\n",
      "Validation Batch 85, Loss: nan\n",
      "Validation Batch 86, Loss: nan\n",
      "Validation Batch 87, Loss: nan\n",
      "Validation Batch 88, Loss: nan\n",
      "Validation Batch 89, Loss: nan\n",
      "Validation Batch 90, Loss: nan\n",
      "Validation Batch 91, Loss: nan\n",
      "Validation Batch 92, Loss: nan\n",
      "Validation Batch 93, Loss: nan\n",
      "Validation Batch 94, Loss: nan\n",
      "Validation Batch 95, Loss: nan\n",
      "Validation Batch 96, Loss: nan\n",
      "Validation Batch 97, Loss: nan\n",
      "Validation Batch 98, Loss: nan\n",
      "Validation Batch 99, Loss: nan\n",
      "Validation Batch 100, Loss: nan\n",
      "Validation Batch 101, Loss: nan\n",
      "Validation Batch 102, Loss: nan\n",
      "Validation Batch 103, Loss: nan\n",
      "Validation Batch 104, Loss: nan\n",
      "Validation Batch 105, Loss: nan\n",
      "Validation Batch 106, Loss: nan\n",
      "Validation Batch 107, Loss: nan\n",
      "Validation Batch 108, Loss: nan\n",
      "Validation Batch 109, Loss: nan\n",
      "Validation Batch 110, Loss: nan\n",
      "Validation Batch 111, Loss: nan\n",
      "Validation Batch 112, Loss: nan\n",
      "Validation Batch 113, Loss: nan\n",
      "Validation Batch 114, Loss: nan\n",
      "Validation Batch 115, Loss: nan\n",
      "Validation Batch 116, Loss: nan\n",
      "Validation Batch 117, Loss: nan\n",
      "Validation Batch 118, Loss: nan\n",
      "Validation Batch 119, Loss: nan\n",
      "Validation Batch 120, Loss: nan\n",
      "Validation Batch 121, Loss: nan\n",
      "Validation Batch 122, Loss: nan\n",
      "Validation Batch 123, Loss: nan\n",
      "Validation Batch 124, Loss: nan\n",
      "Validation Batch 125, Loss: nan\n",
      "Validation Batch 126, Loss: nan\n",
      "Validation Batch 127, Loss: nan\n",
      "Validation Batch 128, Loss: nan\n",
      "Validation Batch 129, Loss: nan\n",
      "Validation Batch 130, Loss: nan\n",
      "Validation Batch 131, Loss: nan\n",
      "Validation Batch 132, Loss: nan\n",
      "Validation Batch 133, Loss: nan\n",
      "Validation Batch 134, Loss: nan\n",
      "Validation Batch 135, Loss: nan\n",
      "Validation Batch 136, Loss: nan\n",
      "Validation Batch 137, Loss: nan\n",
      "Validation Batch 138, Loss: nan\n",
      "Validation Batch 139, Loss: nan\n",
      "Validation Batch 140, Loss: nan\n",
      "Validation Batch 141, Loss: nan\n",
      "Validation Batch 142, Loss: nan\n",
      "Validation Batch 143, Loss: nan\n",
      "Validation Batch 144, Loss: nan\n",
      "Validation Batch 145, Loss: nan\n",
      "Validation Batch 146, Loss: nan\n",
      "Validation Batch 147, Loss: nan\n",
      "Validation Batch 148, Loss: nan\n",
      "Validation Batch 149, Loss: nan\n",
      "Validation Batch 150, Loss: nan\n",
      "Validation Batch 151, Loss: nan\n",
      "Validation Batch 152, Loss: nan\n",
      "Validation Batch 153, Loss: nan\n",
      "Validation Batch 154, Loss: nan\n",
      "Validation Batch 155, Loss: nan\n",
      "Validation Batch 156, Loss: nan\n",
      "Validation Batch 157, Loss: nan\n",
      "Validation Batch 158, Loss: nan\n",
      "Validation Batch 159, Loss: nan\n",
      "Validation Batch 160, Loss: nan\n",
      "Validation Batch 161, Loss: nan\n",
      "Validation Batch 162, Loss: nan\n",
      "Validation Batch 163, Loss: nan\n",
      "Validation Batch 164, Loss: nan\n",
      "Validation Batch 165, Loss: nan\n",
      "Validation Batch 166, Loss: nan\n",
      "Validation Batch 167, Loss: nan\n",
      "Validation Batch 168, Loss: nan\n",
      "Validation Batch 169, Loss: nan\n",
      "Validation Batch 170, Loss: nan\n",
      "Validation Batch 171, Loss: nan\n",
      "Validation Batch 172, Loss: nan\n",
      "Validation Batch 173, Loss: nan\n",
      "Validation Batch 174, Loss: nan\n",
      "Validation Batch 175, Loss: nan\n",
      "Validation Batch 176, Loss: nan\n",
      "Validation Batch 177, Loss: nan\n",
      "Validation Batch 178, Loss: nan\n",
      "Validation Batch 179, Loss: nan\n",
      "Validation Batch 180, Loss: nan\n",
      "Validation Batch 181, Loss: nan\n",
      "Validation Batch 182, Loss: nan\n",
      "Validation Batch 183, Loss: nan\n",
      "Validation Batch 184, Loss: nan\n",
      "Validation Batch 185, Loss: nan\n",
      "Validation Batch 186, Loss: nan\n",
      "Validation Batch 187, Loss: nan\n",
      "Validation Batch 188, Loss: nan\n",
      "Validation Batch 189, Loss: nan\n",
      "Validation Batch 190, Loss: nan\n",
      "Validation Batch 191, Loss: nan\n",
      "Validation Batch 192, Loss: nan\n",
      "Validation Batch 193, Loss: nan\n",
      "Validation Batch 194, Loss: nan\n",
      "Validation Batch 195, Loss: nan\n",
      "Validation Batch 196, Loss: nan\n",
      "Validation Batch 197, Loss: nan\n",
      "Validation Batch 198, Loss: nan\n",
      "Validation Batch 199, Loss: nan\n",
      "Validation Batch 200, Loss: nan\n",
      "Validation Batch 201, Loss: nan\n",
      "Validation Batch 202, Loss: nan\n",
      "Validation Batch 203, Loss: nan\n",
      "Validation Batch 204, Loss: nan\n",
      "Validation Batch 205, Loss: nan\n",
      "Validation Batch 206, Loss: nan\n",
      "Validation Batch 207, Loss: nan\n",
      "Validation Batch 208, Loss: nan\n",
      "Validation Batch 209, Loss: nan\n",
      "Validation Batch 210, Loss: nan\n",
      "Validation Batch 211, Loss: nan\n",
      "Validation Batch 212, Loss: nan\n",
      "Validation Batch 213, Loss: nan\n",
      "Validation Batch 214, Loss: nan\n",
      "Validation Batch 215, Loss: nan\n",
      "Validation Batch 216, Loss: nan\n",
      "Validation Batch 217, Loss: nan\n",
      "Validation Batch 218, Loss: nan\n",
      "Validation Batch 219, Loss: nan\n",
      "Validation Batch 220, Loss: nan\n",
      "Validation Batch 221, Loss: nan\n",
      "Validation Batch 222, Loss: nan\n",
      "Validation Batch 223, Loss: nan\n",
      "Validation Batch 224, Loss: nan\n",
      "Validation Batch 225, Loss: nan\n",
      "Validation Batch 226, Loss: nan\n",
      "Validation Batch 227, Loss: nan\n",
      "Validation Batch 228, Loss: nan\n",
      "Validation Batch 229, Loss: nan\n",
      "Validation Batch 230, Loss: nan\n",
      "Validation Batch 231, Loss: nan\n",
      "Validation Batch 232, Loss: nan\n",
      "Validation Batch 233, Loss: nan\n",
      "Validation Batch 234, Loss: nan\n",
      "Validation Batch 235, Loss: nan\n",
      "Validation Batch 236, Loss: nan\n",
      "Validation Batch 237, Loss: nan\n",
      "Validation Batch 238, Loss: nan\n",
      "Validation Batch 239, Loss: nan\n",
      "Validation Batch 240, Loss: nan\n",
      "Validation Batch 241, Loss: nan\n",
      "Validation Batch 242, Loss: nan\n",
      "Validation Batch 243, Loss: nan\n",
      "Validation Batch 244, Loss: nan\n",
      "Validation Batch 245, Loss: nan\n",
      "Validation Batch 246, Loss: nan\n",
      "Validation Batch 247, Loss: nan\n",
      "Validation Batch 248, Loss: nan\n",
      "Validation Batch 249, Loss: nan\n",
      "Validation Batch 250, Loss: nan\n",
      "Validation Batch 251, Loss: nan\n",
      "Validation Batch 252, Loss: nan\n",
      "Validation Batch 253, Loss: nan\n",
      "Validation Batch 254, Loss: nan\n",
      "Validation Batch 255, Loss: nan\n",
      "Validation Batch 256, Loss: nan\n",
      "Validation Batch 257, Loss: nan\n",
      "Validation Batch 258, Loss: nan\n",
      "Validation Batch 259, Loss: nan\n",
      "Validation Batch 260, Loss: nan\n",
      "Validation Batch 261, Loss: nan\n",
      "Validation Batch 262, Loss: nan\n",
      "Validation Batch 263, Loss: nan\n",
      "Validation Batch 264, Loss: nan\n",
      "Validation Batch 265, Loss: nan\n",
      "Validation Batch 266, Loss: nan\n",
      "Validation Batch 267, Loss: nan\n",
      "Validation Batch 268, Loss: nan\n",
      "Validation Batch 269, Loss: nan\n",
      "Validation Batch 270, Loss: nan\n",
      "Validation Batch 271, Loss: nan\n",
      "Validation Batch 272, Loss: nan\n",
      "Validation Batch 273, Loss: nan\n",
      "Validation Batch 274, Loss: nan\n",
      "Validation Batch 275, Loss: nan\n",
      "Validation Batch 276, Loss: nan\n",
      "Validation Batch 277, Loss: nan\n",
      "Validation Batch 278, Loss: nan\n",
      "Validation Batch 279, Loss: nan\n",
      "Validation Batch 280, Loss: nan\n",
      "Validation Batch 281, Loss: nan\n",
      "Validation Batch 282, Loss: nan\n",
      "Validation Batch 283, Loss: nan\n",
      "Validation Batch 284, Loss: nan\n",
      "Validation Batch 285, Loss: nan\n",
      "Validation Batch 286, Loss: nan\n",
      "Validation Batch 287, Loss: nan\n",
      "Validation Batch 288, Loss: nan\n",
      "Validation Batch 289, Loss: nan\n",
      "Validation Batch 290, Loss: nan\n",
      "Validation Batch 291, Loss: nan\n",
      "Validation Batch 292, Loss: nan\n",
      "Validation Batch 293, Loss: nan\n",
      "Validation Batch 294, Loss: nan\n",
      "Validation Batch 295, Loss: nan\n",
      "Validation Batch 296, Loss: nan\n",
      "Validation Batch 297, Loss: nan\n",
      "Validation Batch 298, Loss: nan\n",
      "Validation Batch 299, Loss: nan\n",
      "Validation Batch 300, Loss: nan\n",
      "Validation Batch 301, Loss: nan\n",
      "Validation Batch 302, Loss: nan\n",
      "Validation Batch 303, Loss: nan\n",
      "Validation Batch 304, Loss: nan\n",
      "Validation Batch 305, Loss: nan\n",
      "Validation Batch 306, Loss: nan\n",
      "Validation Batch 307, Loss: nan\n",
      "Validation Batch 308, Loss: nan\n",
      "Validation Batch 309, Loss: nan\n",
      "Validation Batch 310, Loss: nan\n",
      "Validation Batch 311, Loss: nan\n",
      "Validation Batch 312, Loss: nan\n",
      "Validation Batch 313, Loss: nan\n",
      "Validation Batch 314, Loss: nan\n",
      "Validation Batch 315, Loss: nan\n",
      "Validation Batch 316, Loss: nan\n",
      "Validation Batch 317, Loss: nan\n",
      "Validation Batch 318, Loss: nan\n",
      "Validation Batch 319, Loss: nan\n",
      "Validation Batch 320, Loss: nan\n",
      "Validation Batch 321, Loss: nan\n",
      "Validation Batch 322, Loss: nan\n",
      "Validation Batch 323, Loss: nan\n",
      "Validation Batch 324, Loss: nan\n",
      "Validation Batch 325, Loss: nan\n",
      "Validation Batch 326, Loss: nan\n",
      "Validation Batch 327, Loss: nan\n",
      "Validation Batch 328, Loss: nan\n",
      "Validation Batch 329, Loss: nan\n",
      "Validation Batch 330, Loss: nan\n",
      "Validation Batch 331, Loss: nan\n",
      "Validation Batch 332, Loss: nan\n",
      "Validation Batch 333, Loss: nan\n",
      "Validation Batch 334, Loss: nan\n",
      "Validation Batch 335, Loss: nan\n",
      "Validation Batch 336, Loss: nan\n",
      "Validation Batch 337, Loss: nan\n",
      "Validation Batch 338, Loss: nan\n",
      "Validation Batch 339, Loss: nan\n",
      "Validation Batch 340, Loss: nan\n",
      "Validation Batch 341, Loss: nan\n",
      "Validation Batch 342, Loss: nan\n",
      "Validation Batch 343, Loss: nan\n",
      "Validation Batch 344, Loss: nan\n",
      "Validation Batch 345, Loss: nan\n",
      "Validation Batch 346, Loss: nan\n",
      "Validation Batch 347, Loss: nan\n",
      "Validation Batch 348, Loss: nan\n",
      "Validation Batch 349, Loss: nan\n",
      "Validation Batch 350, Loss: nan\n",
      "Validation Batch 351, Loss: nan\n",
      "Validation Batch 352, Loss: nan\n",
      "Validation Batch 353, Loss: nan\n",
      "Validation Batch 354, Loss: nan\n",
      "Validation Batch 355, Loss: nan\n",
      "Validation Batch 356, Loss: nan\n",
      "Validation Batch 357, Loss: nan\n",
      "Validation Batch 358, Loss: nan\n",
      "Validation Batch 359, Loss: nan\n",
      "Validation Batch 360, Loss: nan\n",
      "Validation Batch 361, Loss: nan\n",
      "Validation Batch 362, Loss: nan\n",
      "Validation Batch 363, Loss: nan\n",
      "Validation Batch 364, Loss: nan\n",
      "Validation Batch 365, Loss: nan\n",
      "Validation Batch 366, Loss: nan\n",
      "Validation Batch 367, Loss: nan\n",
      "Validation Batch 368, Loss: nan\n",
      "Validation Batch 369, Loss: nan\n",
      "Validation Batch 370, Loss: nan\n",
      "Validation Batch 371, Loss: nan\n",
      "Validation Batch 372, Loss: nan\n",
      "Validation Batch 373, Loss: nan\n",
      "Validation Batch 374, Loss: nan\n",
      "Validation Batch 375, Loss: nan\n",
      "Validation Batch 376, Loss: nan\n",
      "Validation Batch 377, Loss: nan\n",
      "Validation Batch 378, Loss: nan\n",
      "Validation Batch 379, Loss: nan\n",
      "Validation Batch 380, Loss: nan\n",
      "Validation Batch 381, Loss: nan\n",
      "Validation Batch 382, Loss: nan\n",
      "Validation Batch 383, Loss: nan\n",
      "Validation Batch 384, Loss: nan\n",
      "Validation Batch 385, Loss: nan\n",
      "Validation Batch 386, Loss: nan\n",
      "Validation Batch 387, Loss: nan\n",
      "Validation Batch 388, Loss: nan\n",
      "Validation Batch 389, Loss: nan\n",
      "Validation Batch 390, Loss: nan\n",
      "Validation Batch 391, Loss: nan\n",
      "Validation Batch 392, Loss: nan\n",
      "Validation Batch 393, Loss: nan\n",
      "Validation Batch 394, Loss: nan\n",
      "Validation Batch 395, Loss: nan\n",
      "Validation Batch 396, Loss: nan\n",
      "Validation Batch 397, Loss: nan\n",
      "Validation Batch 398, Loss: nan\n",
      "Validation Batch 399, Loss: nan\n",
      "Validation Batch 400, Loss: nan\n",
      "Validation Batch 401, Loss: nan\n",
      "Validation Batch 402, Loss: nan\n",
      "Validation Batch 403, Loss: nan\n",
      "Validation Batch 404, Loss: nan\n",
      "Validation Batch 405, Loss: nan\n",
      "Validation Batch 406, Loss: nan\n",
      "Validation Batch 407, Loss: nan\n",
      "Validation Batch 408, Loss: nan\n",
      "Validation Batch 409, Loss: nan\n",
      "Validation Batch 410, Loss: nan\n",
      "Validation Batch 411, Loss: nan\n",
      "Validation Batch 412, Loss: nan\n",
      "Validation Batch 413, Loss: nan\n",
      "Validation Batch 414, Loss: nan\n",
      "Validation Batch 415, Loss: nan\n",
      "Validation Batch 416, Loss: nan\n",
      "Validation Batch 417, Loss: nan\n",
      "Validation Batch 418, Loss: nan\n",
      "Validation Batch 419, Loss: nan\n",
      "Validation Batch 420, Loss: nan\n",
      "Validation Batch 421, Loss: nan\n",
      "Validation Batch 422, Loss: nan\n",
      "Validation Batch 423, Loss: nan\n",
      "Validation Batch 424, Loss: nan\n",
      "Validation Batch 425, Loss: nan\n",
      "Validation Batch 426, Loss: nan\n",
      "Validation Batch 427, Loss: nan\n",
      "Validation Batch 428, Loss: nan\n",
      "Validation Batch 429, Loss: nan\n",
      "Validation Batch 430, Loss: nan\n",
      "Validation Batch 431, Loss: nan\n",
      "Validation Batch 432, Loss: nan\n",
      "Validation Batch 433, Loss: nan\n",
      "Validation Batch 434, Loss: nan\n",
      "Validation Batch 435, Loss: nan\n",
      "Validation Batch 436, Loss: nan\n",
      "Validation Batch 437, Loss: nan\n",
      "Validation Batch 438, Loss: nan\n",
      "Validation Batch 439, Loss: nan\n",
      "Validation Batch 440, Loss: nan\n",
      "Validation Batch 441, Loss: nan\n",
      "Validation Batch 442, Loss: nan\n",
      "Validation Batch 443, Loss: nan\n",
      "Validation Batch 444, Loss: nan\n",
      "Validation Batch 445, Loss: nan\n",
      "Validation Batch 446, Loss: nan\n",
      "Validation Batch 447, Loss: nan\n",
      "Validation Batch 448, Loss: nan\n",
      "Validation Batch 449, Loss: nan\n",
      "Validation Batch 450, Loss: nan\n"
     ]
    }
   ],
   "source": [
    "for batch in range(N_BATCHES):\n",
    "    start_idx = batch * BATCH_SIZE\n",
    "    end_idx = (batch + 1) * BATCH_SIZE\n",
    "\n",
    "    src = enc_val[start_idx:end_idx]\n",
    "    dec_input = dec_input_val[start_idx:end_idx]\n",
    "    dec_target = dec_target_val[start_idx:end_idx]\n",
    "\n",
    "    batch_val_loss = val_step(src, dec_input, transformer)\n",
    "    val_loss += batch_val_loss\n",
    "    \n",
    "    print(f\"Validation Batch {batch}, Loss: {batch_val_loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 훈련 과정 시각화 \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVUAAAEICAYAAADvHGcHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAuNUlEQVR4nO3deXxU9dXH8c/JHhKyBwiEkIQdgQQIhEV2tYoKdWvdTW31wbbiVrV20ZbWrY9tlbaP1r2tVIrWUhUQlYqAVlbZA7JDWJMAISFsSc7zx9zEEALZJrmT5Lxfr3kxc++de0+G8OU3526iqhhjjPEOP7cLMMaYlsRC1RhjvMhC1RhjvMhC1RhjvMhC1RhjvMhC1RhjvMhC1RgfIiJZIrLY7TpM/VmotkIiskNELnK7Dl8nImNEpExEiqo8hrldm/FdAW4XYIwvEJEAVS2pZtZeVU1s8oJMs2UjVVNBRIJF5FkR2es8nhWRYGdenIi8LyJHROSQiCwSET9n3sMiskdECkVkk4iMd6b7iciPRWSriOSLyEwRiXHmhYjIG870IyKyTETan6Ou3iKywFluvYhMdKZnish+EfGvtOxVIrKmFttPFhEVke+KyC7gP/X4vBaIyJMislREjorIv8vX78yf6NR7xFm2d6V5nUXkHRHJdWr7Y5V1PyMih0Vku4hcVml6lohscz7r7SJyU13rNo3LQtVU9lNgKJAOpAFDgJ858x4AcoB4oD3wE0BFpCfwQ2CwqrYFvgHscN5zN/BNYDTQETgM/MmZdxsQCXQGYoHJwPGqBYlIIPAe8CHQzlnndBHpqapLgGPAuEpvuRH4ey22X2400Nupuz5uBW4HEoASYJpTdw/gTeBePJ/ZHOA9EQly/hN4H9gJJAOdgBmV1pkJbALigN8Ar4hHmLP+y5zPejiwqp51m8aiqvZoZQ88oXdRNdO3AhMqvf4GsMN5PhX4N9Ctynu6AQeBi4DAKvOygfGVXicAp/G0nW4HPgf611DrSGA/4Fdp2pvAL5znvwZedZ63xROyXWqx/WRAgdTzbHsMUAYcqfIIc+YvAJ6qtHwf4BTgD/wcmFlpnh+wx1nnMCAXCKhmm1nAlkqv2zh1dgDCnO1fA4S6/Xtkj+ofNlI1lXXEM3oqt9OZBvC/wBbgQ+fr548BVHULntHYL4CDIjJDRMrf0wX4l/P19wiekCvFM9L9GzAPmOG0Gn7jjEqrq2m3qpZVqauT8/zvwNVOm+JqYKWqlv8M59t+ud01fCZ7VTWqyuPYOd6/EwjEM8I847N06t/t1N0Z2KnV93DB859I+fuKnafhzna/jWdUv09EZotIrxrqN03MQtVUthdPEJVLcqahqoWq+oCqpgITgfvLe6eq+ndVvdB5rwJPO+/fjeerauVAClHVPap6WlV/qap98HyNvQLPV+nqaupc3r+tVNceZ9sb8ITXZZz51f+826+0TEMv09a5Sl2ngTyqfJYiIs6ye5y6kkSkzjuKVXWeql6MZ9S9EXip/qWbxmCh2noFOjuLyh8BeL5W/0xE4kUkDngUeANARK4QkW5OOBTgGfGViUhPERnnjBRP4OmLlo8qXwAeF5EuzjriRWSS83ysiPRz+otH8YRR5dFouSVAMfCQiASKyBjgSs7sQf4duAcYBbxVafo5t+9FN4tIHxFpg6dF8raqlgIzgctFZLwzAn8AOImn5bEU2Ac8JSJhzuc/oqYNiUh7EZnk9FZPAkVU/5kZN7ndf7BH0z/w9FS1yuPXQAieHSH7nMc0IMR5z33O+47h2WH1c2d6fzwhUQgcwrMDpqMzzw+4H89Ol0I8PdsnnHk3ONOPAQecbZ3VY3SWvQD4FE+YbwCuqjI/CU+4zK4y/XzbT3Z+7mq36SwzxllvUZXHNc78BcCTzs9/FM8OtbhK77/KqbfAqf+CKjXPAvLxjGynOdOzgMVV6lA8veuESp/DEWf7fdz+fbLHmQ9x/tKMMXUkIguAN1T1ZbdrMb7Dvv4bY4wXWagaY4wX2dd/Y4zxIhupGmOMF7l2QZW4uDhNTk52a/PGGNMgK1asyFPV+KrTXQvV5ORkli9f7tbmjTGmQURkZ3XT7eu/McZ4kYWqMcZ4kYWqMcZ4kV3535gmcvr0aXJycjhx4oTbpZg6CAkJITExkcDA6i6idjYLVWOaSE5ODm3btiU5ORnPdWmMr1NV8vPzycnJISUlpVbvsa//xjSREydOEBsba4HajIgIsbGxdfp2UetQFRF/EflSRN6vZl6wiPxDRLaIyBIRSa51Bca0IhaozU9d/87qMlK9B8+V06vzXeCwqnYDfs/XFyn2qn+v2sP0JdUeGmaMMT6hVqEqIonA5cC5LnE2CfiL8/xtYLw0wn/Jc9fu5/kFW729WmNahfz8fNLT00lPT6dDhw506tSp4vWpU6fO+97ly5czZcqUOm0vOTmZvLy8hpTcLNV2R9WzwEN4bqxWnU449+pR1RIRKcBzh8wzPlERuRO4EyApKanOxWamxvDB+v3kHC4mMbpNnd9vTGsWGxvLqlWrAPjFL35BeHg4P/rRjyrml5SUEBBQfSRkZGSQkZHRFGU2ezWOVEXkCuCgqq5o6MZU9UVVzVDVjPj4s06ZrdHQ1FgAlmw71NBSjDFAVlYWkydPJjMzk4ceeoilS5cybNgwBgwYwPDhw9m0aRMACxYs4IorrgA8gXz77bczZswYUlNTmTZtWq23t2PHDsaNG0f//v0ZP348u3btAuCtt96ib9++pKWlMWrUKADWr1/PkCFDSE9Pp3///mzevNnLP33jqM1IdQQwUUQm4LndRoSIvKGqN1daZg+em5rlOPc6isRzmwiv6tm+LVFtAvliWz7XDEr09uqNaTK/fG89G/Ye9eo6+3SM4LErL6jz+3Jycvj888/x9/fn6NGjLFq0iICAAD7++GN+8pOf8M9//vOs92zcuJFPPvmEwsJCevbsyV133VWr4zjvvvtubrvtNm677TZeffVVpkyZwqxZs5g6dSrz5s2jU6dOHDlyBIAXXniBe+65h5tuuolTp05RWlpa55/NDTWOVFX1EVVNVNVk4HrgP1UCFeBd4Dbn+bXOMl6/UKufnzAkOYYl222kaoy3XHfddfj7+wNQUFDAddddR9++fbnvvvtYv359te+5/PLLCQ4OJi4ujnbt2nHgwIFabeu///0vN954IwC33HILixcvBmDEiBFkZWXx0ksvVYTnsGHDeOKJJ3j66afZuXMnoaGhDf1Rm0S9D/4XkanAclV9F3gF+JuIbMFz87frvVTfWYamxvLhhgPsOXKcTlHN40M2pqr6jCgbS1hYWMXzn//854wdO5Z//etf7NixgzFjxlT7nuDg4Irn/v7+lJSUNKiGF154gSVLljB79mwGDRrEihUruPHGG8nMzGT27NlMmDCBP//5z4wbN65B22kKdTr4X1UXqOoVzvNHnUBFVU+o6nWq2k1Vh6jqtsYoFir3Vb3eXTCm1SsoKKBTp04AvP76615f//Dhw5kxw3N38enTpzNy5EgAtm7dSmZmJlOnTiU+Pp7du3ezbds2UlNTmTJlCpMmTWLNmjVer6cxNLszqnp1aEtkaKDtrDKmETz00EM88sgjDBgwoMGjT4D+/fuTmJhIYmIi999/P3/4wx947bXX6N+/P3/729947rnnAHjwwQfp168fffv2Zfjw4aSlpTFz5kz69u1Leno669at49Zbb21wPU3BtXtUZWRkaH0vUn3HX5fz1YFCPn1wrJerMqbxZGdn07t3b7fLMPVQ3d+diKxQ1bOOM2t2I1XwtAB25hez98hxt0sxxpgzNMtQzUyJAWDJduurGmN8S7MM1d4JEUSEBFhf1Rjjc5plqPr7CUNSYvnCjgAwxviYZhmqAENTY9iRX8z+AruKujHGdzTjUHWOV7W+qjHGhzTbUO2dEEHbkABrARhTS2PHjmXevHlnTHv22We56667zvmeMWPGUH7o44QJEyrOy6/sF7/4Bc8888x5tz1r1iw2bNhQ8frRRx/l448/rkP11at8oRdf0WxD1d+5DsAXtrPKmFq54YYbKs5mKjdjxgxuuOGGWr1/zpw5REVF1WvbVUN16tSpXHTRRfVal69rtqEKnhbA9rxjHDhqfVVjanLttdcye/bsigtS79ixg7179zJy5EjuuusuMjIyuOCCC3jssceqfX/li04//vjj9OjRgwsvvLDi8oAAL730EoMHDyYtLY1rrrmG4uJiPv/8c959910efPBB0tPT2bp1K1lZWbz99tsAzJ8/nwEDBtCvXz9uv/12Tp48WbG9xx57jIEDB9KvXz82btxY65/1zTffrDhD6+GHHwagtLSUrKws+vbtS79+/fj9738PwLRp0+jTpw/9+/fn+usbftmSZn031fK+6hfb8pmU3snlaoypg7k/hv1rvbvODv3gsqfOOTsmJoYhQ4Ywd+5cJk2axIwZM/jWt76FiPD4448TExNDaWkp48ePZ82aNfTv37/a9axYsYIZM2awatUqSkpKGDhwIIMGDQLg6quv5o477gDgZz/7Ga+88gp33303EydO5IorruDaa689Y10nTpwgKyuL+fPn06NHD2699Vaef/557r33XgDi4uJYuXIl//d//8czzzzDyy+f6+YjX9u7dy8PP/wwK1asIDo6mksuuYRZs2bRuXNn9uzZw7p16wAqWhlPPfUU27dvJzg4uNr2Rl0165Fqn44RtA0OsBaAMbVUuQVQ+av/zJkzGThwIAMGDGD9+vVnfFWvatGiRVx11VW0adOGiIgIJk6cWDFv3bp1jBw5kn79+jF9+vRzXjqw3KZNm0hJSaFHjx4A3HbbbSxcuLBi/tVXXw3AoEGD2LFjR61+xmXLljFmzBji4+MJCAjgpptuYuHChaSmprJt2zbuvvtuPvjgAyIiIgDP9Qluuukm3njjjXPe+aAumvVI1d9PGJwSY0cAmObnPCPKxjRp0iTuu+8+Vq5cSXFxMYMGDWL79u0888wzLFu2jOjoaLKysup0S+bKsrKymDVrFmlpabz++ussWLCgQfWWX2LQG5cXjI6OZvXq1cybN48XXniBmTNn8uqrrzJ79mwWLlzIe++9x+OPP87atWsbFK7NeqQKnuNVt+Ue46D1VY2pUXh4OGPHjuX222+vGKUePXqUsLAwIiMjOXDgAHPnzj3vOkaNGsWsWbM4fvw4hYWFvPfeexXzCgsLSUhI4PTp00yfPr1ietu2bSksLDxrXT179mTHjh1s2bIFgL/97W+MHj26QT/jkCFD+PTTT8nLy6O0tJQ333yT0aNHk5eXR1lZGddccw2//vWvWblyJWVlZezevZuxY8fy9NNPU1BQQFFRUYO236xHqgCZKU5fdfshJqZ1dLkaY3zfDTfcwFVXXVXRBkhLS2PAgAH06tWLzp07M2LEiPO+f+DAgXz7298mLS2Ndu3aMXjw4Ip5v/rVr8jMzCQ+Pp7MzMyKIL3++uu54447mDZtWsUOKoCQkBBee+01rrvuOkpKShg8eDCTJ0+u088zf/58EhO/vr3SW2+9xVNPPcXYsWNRVS6//HImTZrE6tWr+c53vkNZWRkATz75JKWlpdx8880UFBSgqkyZMqXeRziUa5aX/quspLSM9KkfMSm9I49f1c8LlRnTOOzSf81Xi7/0X2UB/n4MTo62kwCMMT6h2YcqQGZqLFtzj3Gw0Pqqxhh31RiqIhIiIktFZLWIrBeRX1azTJaI5IrIKufxvcYpt3rlx6sutbusGh/nVrvN1F9d/85qM1I9CYxT1TQgHbhURIZWs9w/VDXdedR8hK4X9e0YQViQv7UAjE8LCQkhPz/fgrUZUVXy8/MJCQmp9Xtq3Puvnt+A8mMMAp2HT/1WBPj7kWHXATA+LjExkZycHHJzc90uxdRBSEjIGUcX1KRWh1SJiD+wAugG/ElVl1Sz2DUiMgr4CrhPVXdXs547gTsBkpKSal1kbQxNjeXpDzaSV3SSuPDgmt9gTBMLDAwkJSXF7TJMI6vVjipVLVXVdCARGCIifass8h6QrKr9gY+Av5xjPS+qaoaqZsTHxzeg7LMNTXXuW2WjVWOMi+q0919VjwCfAJdWmZ6vqiedly8Dg7xSXR307RRJG+urGmNcVpu9//EiEuU8DwUuBjZWWSah0suJQLYXa6yVQKevatcBMMa4qTYj1QTgExFZAywDPlLV90VkqoiUX55minO41WpgCpDVOOWe39DUGL46UERe0cmaFzbGmEZQm73/a4AB1Ux/tNLzR4BHvFta3ZVfB2Dp9kNM6JdQw9LGGON9LeKMqnL9EyMJDfRnifVVjTEuaVGh6umrRtvxqsYY17SoUAXP8aqbDhRy6Ngpt0sxxrRCLTBUPcerLrWjAIwxLmhxodqvUxShgf7WAjDGuKLFhWpQgB+Dutj1VY0x7mhxoQqeFsDG/YUctr6qMaaJtdBQ9RyvusSur2qMaWItMlT7J0YREuhnLQBjTJNrkaFa3le1kaoxpqm1yFAFGJoSy8b9RzlSbH1VY0zTabGhmpkai6r1VY0xTavFhmpa50iCA/zsotXGmCbVYkM1OMDfjlc1xjS5Fhuq4LkUYPb+oxQUn3a7FGNMK9GiQ3VoagyqsHSHtQCMMU2jRYdqWucoggPseFVjTNNp0aEaEujPgKQoC1VjTJNp0aEKnlNWN+w7SsFx66saYxpfbe6mGiIiS0VktXNzv19Ws0ywiPxDRLaIyBIRSW6UauthqHO86jI7XtUY0wRqM1I9CYxT1TQgHbhURIZWWea7wGFV7Qb8Hnjaq1U2QHrnKIKsr2qMaSI1hqp6FDkvA52HVllsEvAX5/nbwHgREa9V2QAhgf4M6BxlZ1YZY5pErXqqIuIvIquAg8BHqrqkyiKdgN0AqloCFACx1aznThFZLiLLc3NzG1R4XQxNjWX93gLrqxpjGl2tQlVVS1U1HUgEhohI3/psTFVfVNUMVc2Ij4+vzyrqJTM1hjKF5Xa8qjGmkdVp77+qHgE+AS6tMmsP0BlARAKASMBnmpgDk6IJ8vezFoAxptHVZu9/vIhEOc9DgYuBjVUWexe4zXl+LfAfVa3ad3VNSKA/6Xa8qjGmCdRmpJoAfCIia4BleHqq74vIVBGZ6CzzChArIluA+4EfN0659Tc0JYZ1ewo4esL6qsaYxhNQ0wKqugYYUM30Rys9PwFc593SvGtoaizT/rOFFTsOM7ZXO7fLMca0UC3+jKpyA5y+qrUAjDGNqdWEamiQP2mdIy1UjTGNqtWEKnhaAOv2HqXQ+qrGmEbS6kK1tExZvvOw26UYY1qoVhWqA5OiCfQXawEYYxpNqwrV0CB/0hKj7GaAxphG06pCFTwtgLV7Cig6WeJ2KcaYFqjVhWpmaoynr2rXATDGNIJWF6qDukQT4Cd8YS0AY0wjaHWh2iYogLTOUSzZbjurjDHe1+pCFSAzJYY1OQUcs76qMcbLWmWo2vGqxpjG0ipDtbyvusSOVzXGeFmrDNWw4AD6Jdp1AIwx3tcqQxU8LYA1OQUUn7K+qjHGe1p1qJaUKSusr2qM8aJWG6oZXaLx9xPmZx90uxRjTAvSakM1LDiAb6Z34u9LdrH7ULHb5RhjWoja3Pivs4h8IiIbRGS9iNxTzTJjRKRARFY5j0erW5ev+dE3eiACz3y4ye1SjDEtRG1GqiXAA6raBxgK/EBE+lSz3CJVTXceU71aZSNJiAzleyNT+PeqvazefcTtcowxLUCNoaqq+1R1pfO8EMgGOjV2YU1l8uiuxIYF8ficbHzortrGmGaqTj1VEUnGc2fVJdXMHiYiq0Vkrohc4I3imkLbkEDuvag7S7cf4mPbaWWMaaBah6qIhAP/BO5V1aNVZq8EuqhqGvAHYNY51nGniCwXkeW5ubn1LNn7rh+SRGp8GE/OzeZ0aZnb5RhjmrFahaqIBOIJ1Omq+k7V+ap6VFWLnOdzgEARiatmuRdVNUNVM+Lj4xtYuvcE+vvxyGW92ZZ7jBnLdrtdjjGmGavN3n8BXgGyVfV351img7McIjLEWW+zOgf0ot7tGJISw7MffWV3WzXG1FttRqojgFuAcZUOmZogIpNFZLKzzLXAOhFZDUwDrtdmttdHRPjphN7kHzvFC59udbscY0wzFVDTAqq6GJAalvkj8EdvFeWWtM5RTEzryMuLtnPz0C4kRIa6XZIxpplptWdUncuD3+iJKjwz7yu3SzHGNEMWqlV0jmnDd0Yk886XOazfW+B2OcaYZsZCtRrfH9uNyNBAnrATAowxdWShWo3I0ECmjOvOZ1vyWfCV7xxPa4zxfRaq53Dz0C50iW3Dk3OyKbETAowxtWSheg5BAX48fGkvvjpQxNsrctwuxxjTTFionsdlfTswMCmK3370ld3O2hhTKxaq5yEi/PTyPuQWnuSlRdvcLscY0wxYqNZgUJdoJvTrwJ8/3cbBoyfcLscY4+MsVGvhoW/0oqSsjN99ZCcEGGPOz0K1FpLjwrhlaDIzl+9m0/5Ct8sxxvgwC9VauntcN8KCA3hybrbbpRhjfJiFai1FhwVx97huLNiUy+LNeW6XY4zxURaqdXDrsGQSo0N5fE42pWV2+qox5mwWqnUQEujPg9/oSfa+o/zryz1ul2OM8UEWqnV0Zf+OpCVG8tsPN3H8VKnb5RhjfIyFah35+Qk/mdCbfQUnePWz7W6XY4zxMRaq9ZCZGsvFfdrz/IKt5BWddLscY4wPsVCtpx9f1ovjp0t57uPNbpdijPEhtbmbamcR+URENojIehG5p5plRESmicgWEVkjIgMbp1zf0TU+nBuHJPH3pbvYcrDI7XKMMT6iNiPVEuABVe0DDAV+ICJ9qixzGdDdedwJPO/VKn3UPRd1JzTQn6fmbnS7FGOMj6gxVFV1n6qudJ4XAtlApyqLTQL+qh5fAFEikuD1an1MXHgwd43pysfZB/hiW77b5RhjfECdeqoikgwMAJZUmdUJ2F3pdQ5nBy8icqeILBeR5bm5LeM2Jd+9MIWEyBCemJNNmZ0QYEyrV+tQFZFw4J/Avap6tD4bU9UXVTVDVTPi4+PrswqfExLoz48u6cmanALeW7PX7XKMMS6rVaiKSCCeQJ2uqu9Us8geoHOl14nOtFbhqgGd6JMQwW8+2MSJ03ZCgDGtWW32/gvwCpCtqr87x2LvArc6RwEMBQpUdZ8X6/Rpfn7Czy7vzZ4jx3ngrdV2o0BjWrGAWiwzArgFWCsiq5xpPwGSAFT1BWAOMAHYAhQD3/F6pT5ueLc4fjKhF0/M2Uiwvx/PXJeGn5+4XZYxponVGKqquhg4bzqoqgI/8FZRzdWdo7py8nQZv/3oK4ID/Xjiqn54BvrGmNaiNiNVUwd3j+/OiZJS/vTJVoID/Hnsyj4WrMa0IhaqjeBHl/Tk5OkyXl68neAAP358WS8LVmNaCQvVRuC5tXVvTpaU8eeF2wgO9Of+i3u4XZYxpglYqDYSEeGXEy/gZEkp0+ZvJiTQj++P6eZ2WcaYRmah2oj8/IQnr+7PyZIyfvPBJoID/PnuhSlul2WMaUQWqo3M30/47XVpnCop41fvbyA4wI+bh3ZxuyxjTCOx66k2gQB/P567fgDje7XjZ7PWMXP57prfZIxplixUm0hQgB9/umkgI7vH8fA/1/DvVa3mLF5jWhUL1SYUEujPi7dkMCQ5hvtnruaDda3mTF5jWg0L1SYWGuTPK1mDSUuM5O43v+Q/Gw+4XZIxxossVF0QHhzA67cPoVeHCCa/sZLFm/PcLskY4yUWqi6JCAnkr7cPITUujO/9dRlL7M4BxrQIFqouig4L4o3vZZIY3YbbX1/Gip2H3S7JGNNAFqouiwsPZvr3MolrG0zWa0tZm1PgdknGmAawUPUB7SNC+PsdQ4kICeSWV5eQva9ed6sxxvgAC1Uf0SkqlDfvGEpIgD83v7yELQcL3S7JGFMPFqo+JCm2DdPvyEREuPGlJezIO+Z2ScaYOrJQ9TFd48OZ/r1MTpeWcf2LX/D5FjvcypjmxELVB/Xs0Jbp3xtKSKAfN768hIffXkPB8dNul2WMqYXa3E31VRE5KCLrzjF/jIgUiMgq5/Go98tsffp0jOCDe0fxP6NTeXtlDhf/7lM+WLff7bKMMTWozUj1deDSGpZZpKrpzmNqw8sy4LlWwCOX9WbW90cQGx7M5DdW8P3pKzhYeMLt0owx51BjqKrqQuBQE9RizqFfYiTv/nAED36jJx9nH+Ti3y3kreW78dzE1hjjS7zVUx0mIqtFZK6IXHCuhUTkThFZLiLLc3NzvbTp1iHQ348fjO3G3HtG0qN9OA++vYZbX13K7kPFbpdmjKlEajPaEZFk4H1V7VvNvAigTFWLRGQC8Jyqdq9pnRkZGbp8+fJ6lGzKypTpS3by1NyNlCk8cEkPvjMiBX8/u2OrMU1FRFaoakbV6Q0eqarqUVUtcp7PAQJFJK6h6zXn5ucn3DIsmY/uH82wrrH8enY21zz/OZv22wkDxritwaEqIh3Euam9iAxx1mmXXGoCHaNCeeW2DJ67Pp1dh4q54g+L+P1HX3GypNTt0oxptWq88Z+IvAmMAeJEJAd4DAgEUNUXgGuBu0SkBDgOXK+2B6XJiAiT0jtxYbc4pr6/gefmb2bO2n08fW1/BiZFu12eMa1OrXqqjcF6qo3jPxsP8NN/rWP/0RNkDU/mR5f0JCzYbpprjLc1Wk/V+JZxvdrz4X2juDmzC699toNvPLuQRZvtSAtjmoqFagvUNiSQX32zLzP/ZxhB/n7c8spSfvTWao4Un3K7NGNaPAvVFmxISgxz7hnJD8Z25V9f7mHUbz7hmXmbyCs66XZpxrRY1lNtJbL3HWXa/M18sH4/Qf5+XJeRyJ0ju5IU28bt0oxpls7VU7VQbWW25Rbx4sJtvLNyDyVlZVzevyOTR6dyQcdIt0szplmxUDVnOHD0BK8u3s70JbsoOlnC6B7xTB7dlaGpMTiHHRtjzsNC1VSr4Php3vhiJ699tp28olOkd45i8uiuXNKnPX522qsx52Shas7rxOlS3l6Rw4sLt7HrUDGp8WFMHtWVbw7oRFCA7c80pioLVVMrJaVlzF23n+cXbGXDvqN0iAjhuxemcENmEuF2EoExFSxUTZ2oKos25/H8gq38d1s+ESEB3DosmawRycSFB7tdnjGus1A19bZq9xFeWLCVeRs8h2N9K6Mzd4xMtcOxTKtmoWoabGtuES9+uo13vsyhtEwZ16sdV6Z15OI+7WkTZK0B07pYqBqv2V9wgtc/38GsL/ew/+gJQgP9Gd+7HRPTOjK6ZzzBAf5ul2hMo7NQNV5XVqYs23GId1fvZc7afRwuPk3bkAAuvaADV6Z1ZHjXWAL87cgB0zJZqJpGdbq0jM+25PHe6n18uH4/hSdLiA0LYkK/BCamd2RQUrQd92paFAtV02ROnC5lwaZc3lu9l4+zD3CypIyOkSFckdaRK/t3pG+nCDtryzR7FqrGFUUnS/h4wwHeW72XhZtzOV2qpMSFcWVaRyamJdCtXVu3SzSmXixUjeuOFJ/ig3X7eW/NXv67NZ8yhd4JEVyZlsCV/TvSOcYO0TLNh4Wq8SkHC08wZ80+3l29l5W7jgDQJyGCi3q3Y3zv9vTrFGk9WOPT6h2qIvIqcAVwUFX7VjNfgOeACUAxkKWqK2sqyELVlNt9qJg5a/fxcfYBVuw8TJlCfNtgxvdqx7he7biwe5wdB2t8TkNCdRRQBPz1HKE6AbgbT6hmAs+pamZNBVmomuocPnaKBV8d5OPsgyzclEvhyRKCA/wY3jWW8b3bM753OxIiQ90u05iGff0XkWTg/XOE6p+BBar6pvN6EzBGVfedb50WqqYmp0rKWLbjEPOzDzJ/4wF25hcD1iYwvqExQ/V94ClVXey8ng88rKpnJaaI3AncCZCUlDRo586ddf05TCulqmzNLeLj7IPMr9ImGNezHeN7W5vANK1zhWqT/gaq6ovAi+AZqTbltk3zJiJ0a9eWbu3aMnl01zPaBHPW7uMfy3cTFODHiK6xjOvdnvG92tExytoEpul5I1T3AJ0rvU50phnTaKLDgrhqQCJXDUg8q03wyax1/Bzo3i6ckd3jGdUjjsyUWEKD7JoEpvF54+v/5cAP+XpH1TRVHVLTOq2nahpDeZvgPxsPsmhzHku3H+JkSRlB/n4MTolmZPd4RnaPo3eHCOvFmgZpyN7/N4ExQBxwAHgMCARQ1RecQ6r+CFyK55Cq71TXT63KQtU0hROnS1m6/RCLNueyaHMeG/cXAhAXHsSF3eIY1SOeC7vH0a5tiMuVmubGDv43Bs9dZBdvzmPh5lwWb84j/9gpAHp1aMuoHp5R7ODkGEICrVVgzs9C1ZgqysqUDfuOsmhzHos257J8x2FOlZYRHODHkJQYRnWPZ1SPeHq0D7cLwJizWKgaU4PiUyUs2X6IRV95RrJbDhYB0K5tMBd2j2NE1zhGdIujQ6S1CoyPHFJljC9rExTA2J7tGNuzHQB7jxxn8eY8Pt2cyycbD/LOSs9BLV3jw7iwWxzDu8UxNDWWyNBAN8s2PsZGqsbUQlmZkr3/KJ9tyeOzLfks3X6I46dL8RPolxjFiK6xjOgWx6Au0daPbSXs678xXnSqpIwvdx3ms635fL4ljy93H6G0TAkO8CMjOZrhXeO4sFscfTtF4m+HbrVIFqrGNKKikyUs3Z7PZ1vy+WzL14duRYQEMMwZxQ7vGkfX+DDb6dVCWE/VmEYUHhzAuF7tGderPQB5RSf5fGs+n23O47OtecxbfwCADhEhDO8Wy9CUWAYlR5MaZyHb0thI1ZgmsCu/mMVbPAH73635HHKOj41qE8igpGgGdolmUJdo0hKj7HTaZsK+/hvjI8rKlG15x1i58zArdh5mxa7DFYdv+fsJfRIiGNTl66DtGBlio1kfZKFqjA87UnyKL3cd8YTszsOs2n2E46dLAU/LoHLI9kmIICjAz+WKjfVUjfFhUW2CGNurHWN7eY6RLSktY+P+woqQXbHzMLPXeq77HhzgR1piVEXIDkyKIjY82M3yTSU2UjWmmdhfcIKVu74O2fV7Czhd6vn3mxAZQs8ObenZoS29OrSlZ/sIurYLIzjA+rONxUaqxjRzHSJDmNAvgQn9EgDPFbjW7ilg5c7DZO87ysb9hXy2Ja8iaAP8hJS4sK+DtkMEvTq0JTE61Hq0jchC1ZhmKiTQn8HJMQxOjqmYdrq0jO15x9i4v5BN+4+yaX8hq3Yf4f01X98yLjw4gB7twytCtjx0o9oEufFjtDj29d+YVqDwxGm+OlDEJidsN+4vZNOBQo4Un65Ypn1EcEXQdosPp2u7cLrFhxPZxq5tUB37+m9MK9Y2JJBBzo6tcqrKwcKTFaNaz5+FvP55PqdKyiqWiwsPoqsTsl3jw+nWLpyu8WF0jAy1uydUw0LVmFZKRGgfEUL7iBBG94ivmF5apuQcLmZrbhFbDhax9eAxtuYWMWftvjNGtqGB/qTGh3kCtzxs24WRHBvWqi8qY6FqjDmDv5/QJTaMLrFhFafdgmdke+jYKU/Q5h6rCN2Vuw7z7uq9Fcv5CXSOaeOErSd0U+PDSYkLIy48qMXvJKtVqIrIpcBzgD/wsqo+VWV+FvC/fH0X1T+q6sterNMY4zIRITY8mNjwYDJTY8+Yd/xUKdvyPGHrCd0ith4sYvGWvDNaCW2DA0iJDyM1LoyUuPBKz8MIC24ZY7wafwoR8Qf+BFwM5ADLRORdVd1QZdF/qOoPG6FGY4yPCw3y54KOkVzQMfKM6aVlyp7Dx9mWV8T2vGNszzvGttxjLNtxmFmr9p6xbPuIYFLiwkiND68I2tT4cBKjQwn0bz5nkNXmv4YhwBZV3QYgIjOASUDVUDXGmDP4+wlJsW1Iim3DmJ5nzjt+qpQd+cfOCNtteUXMXrOPguNf924D/ISkmDZOyHraEkkxbUiKaUMnHwzc2oRqJ2B3pdc5QGY1y10jIqOAr4D7VHV31QVE5E7gToCkpKS6V2uMaTFCg/zpnRBB74SIs+YdPnaKbXlFbMs9M3QXVWkn+Al0jAqtCNnOzp9dYj1/RoYGNnkP11tNjPeAN1X1pIj8D/AXYFzVhVT1ReBF8Byn6qVtG2NamOiwIAaFxTCoS8wZ08vKlAOFJ9iVX8zOQ8XsPlTMLufxcfYB8opOnbF825CAisAtD93ywO0Y1Tij3NqE6h6gc6XXiXy9QwoAVc2v9PJl4DcNL80YY87k5yckRIaSEBl61s4ygGMnS9h9uJhd+Z6gLQ/drw4UMn/jwWpHuXeN6cpNmV28VmNtQnUZ0F1EUvCE6fXAjZUXEJEEVS0/D24ikO21Co0xppbCggPo1SGCXh3ObimUlXlOdtiZf+yMwI3z8hW+agxVVS0RkR8C8/AcUvWqqq4XkanAclV9F5giIhOBEuAQkOXVKo0xpoH8/IQOkSF0iAypdpTrLXbuvzHG1MO5zv33rWMRjDGmmbNQNcYYL7JQNcYYL7JQNcYYL7JQNcYYL7JQNcYYL7JQNcYYL3LtOFURyQV21uOtcUCel8vxJl+uz5drA9+uz5drA9+uz5drg/rX10VV46tOdC1U60tElld3wK2v8OX6fLk28O36fLk28O36fLk28H599vXfGGO8yELVGGO8qDmG6otuF1ADX67Pl2sD367Pl2sD367Pl2sDL9fX7Hqqxhjjy5rjSNUYY3yWhaoxxnhRswpVEblURDaJyBYR+bHb9ZQTkc4i8omIbBCR9SJyj9s1VSUi/iLypYi873YtVYlIlIi8LSIbRSRbRIa5XVNlInKf8/e6TkTeFJEQF2t5VUQOisi6StNiROQjEdns/BntY/X9r/N3u0ZE/iUiUb5UX6V5D4iIikhcQ7bRbEJVRPyBPwGXAX2AG0Skj7tVVSgBHlDVPsBQ4Ac+VFu5e/Dd29w8B3ygqr2ANHyoThHpBEwBMlS1L567X1zvYkmvA5dWmfZjYL6qdgfmO6/d8jpn1/cR0FdV++O52/IjTV1UJa9zdn2ISGfgEmBXQzfQbEIVGAJsUdVtqnoKmAFMcrkmAFR1n6qudJ4X4gmFTu5W9TURSQQux3NTRp8iIpHAKOAVAFU9papHXC3qbAFAqIgEAG2AvW4VoqoL8dyyqLJJeO5gjPPnN5uypsqqq09VP1TVEuflF3huHuqKc3x+AL8HHgIavOe+OYVqJ2B3pdc5+FBwlRORZGAAsMTlUip7Fs8vTFkNy7khBcgFXnPaEy+LSJjbRZVT1T3AM3hGMPuAAlX90N2qztK+0o039wPt3SymBrcDc90uojIRmQTsUdXV3lhfcwpVnyci4cA/gXtV9ajb9QCIyBXAQVVd4XYt5xAADASeV9UBwDHc/fp6Bqc/OQlP+HcEwkTkZnerOjf1HCPpk8dJishP8bTKprtdSzkRaQP8BHjUW+tsTqG6B+hc6XWiM80niEggnkCdrqrvuF1PJSOAiSKyA0/LZJyIvOFuSWfIAXJUtXxk/zaekPUVFwHbVTVXVU8D7wDDXa6pqgMikgCe28UDB12u5ywikgVcAdykvnVwfFc8/2Gudv6NJAIrRaRDfVfYnEJ1GdBdRFJEJAjPzoJ3Xa4JABERPD3BbFX9ndv1VKaqj6hqoqom4/nM/qOqPjPSUtX9wG4R6elMGg9scLGkqnYBQ0WkjfP3PB4f2pHmeBe4zXl+G/BvF2s5i4hciqf9NFFVi92upzJVXauq7VQ12fk3kgMMdH4v66XZhKrT6P4hMA/PL/VMVV3vblUVRgC34BkFrnIeE9wuqhm5G5guImuAdOAJd8v5mjOCfhtYCazF82/GtdMuReRN4L9ATxHJEZHvAk8BF4vIZjwj66d8rL4/Am2Bj5x/Gy/4WH3e3YZvjcSNMaZ5azYjVWOMaQ4sVI0xxossVI0xxossVI0xxossVI0xxossVI0xxossVI0xxov+HzauFBsAhKH0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Losses over Epochs')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 평가\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    tokens = mecab.morphs(sentence)\n",
    "    tokenized_sentence = [word_to_index.get(token, word_to_index[\"<unk>\"]) for token in tokens]\n",
    "    tokenized_sentence = tf.expand_dims(tokenized_sentence, 0)\n",
    "    dec_input = tf.expand_dims([word_to_index[\"<start>\"]], 0)\n",
    "    output_sequence = []\n",
    "\n",
    "    for _ in range(40):  # Assuming MAX_LENGTH is 40\n",
    "        enc_mask, dec_enc_mask, dec_mask = generate_masks(tokenized_sentence, dec_input)\n",
    "        predictions, _, _, _ = transformer(tokenized_sentence, dec_input, enc_mask, dec_enc_mask, dec_mask)\n",
    "        predicted_id = tf.argmax(predictions[:, -1, :], axis=-1)\n",
    "        predicted_id = tf.cast(predicted_id, tf.int32)\n",
    "        predicted_id = tf.expand_dims(predicted_id, axis=0)\n",
    "        dec_input = tf.concat([dec_input, predicted_id], axis=-1)\n",
    "        if predicted_id.numpy()[0][0] == word_to_index[\"<end>\"]:\n",
    "            break\n",
    "    translated_sentence = ' '.join([index_to_word[index] for index in dec_input.numpy()[0] if index != word_to_index[\"<start>\"]])\n",
    "\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    \"지루하다, 놀러가고 싶어.\",\n",
    "    \"오늘 일찍 일어났더니 피곤하다.\",\n",
    "    \"간만에 여자친구랑 데이트 하기로 했어.\",\n",
    "    \"집에 있는다는 소리야.\",\n",
    "    \"내일 밤 늦게 만나서 얘기나누자.\",\n",
    "    \"오늘은 너무 행복한 날이야.\",\n",
    "    \"죽고싶니? 너 머하는 놈이야?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = []\n",
    "for idx, sentence in enumerate(samples):\n",
    "    translation = evaluate(sentence)\n",
    "    translations.append(f\"> {idx+1}. {translation}\")\n",
    "\n",
    "# for translation in translations:\n",
    "#     print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"> \"\n",
    "\n",
    "hyperparameters = [\n",
    "    f\"n_layers: {n_layers}\",\n",
    "    f\"d_model: {d_model}\",\n",
    "    f\"n_heads: {n_heads}\",\n",
    "    f\"d_ff: {d_ff}\",\n",
    "    f\"dropout: {dropout}\"\n",
    "]\n",
    "hyperparameters = [prefix + item for item in hyperparameters]\n",
    "\n",
    "training_parameters = [\n",
    "    f\"Warmup Steps: {warmup_steps}\",\n",
    "    f\"Batch Size: {BATCH_SIZE}\",\n",
    "    f\"Epoch At: {EPOCHS}\"\n",
    "]\n",
    "training_parameters = [prefix + item for item in training_parameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = \"\\n\\n\".join([\"Translations\"] + translations + hyperparameters + training_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "\n",
      "> 1. 답답 하 다 , 놀 러 가 고 싶 어 . <end>\n",
      "\n",
      "> 2. 아침 일찍 잘못 더니 피곤 하 다 . <end>\n",
      "\n",
      "> 3. 간만에 여자 친구 랑 데이트 하 기 로 했 어 . <end>\n",
      "\n",
      "> 4. 집 에 있 ㄴ다는 거 야 . <end>\n",
      "\n",
      "> 5. 내일 밤 늦 게 만나 서 서 얘기 자 . <end>\n",
      "\n",
      "> 6. 오늘 은데 너무 행복 한 날 이 야 . <end>\n",
      "\n",
      "> 7. 죽 고 싶 니 ? 악몽 이 시키 는 데 이 야 ? <end>\n",
      "\n",
      "> n_layers: 1\n",
      "\n",
      "> d_model: 368\n",
      "\n",
      "> n_heads: 8\n",
      "\n",
      "> d_ff: 1024\n",
      "\n",
      "> dropout: 0.2\n",
      "\n",
      "> Warmup Steps: 1000\n",
      "\n",
      "> Batch Size: 64\n",
      "\n",
      "> Epoch At: 15\n"
     ]
    }
   ],
   "source": [
    "print(final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 7. 성능 측정하기**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU Score로 성능 측정하기\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_single(sentence, reference_sentence, verbose=True):\n",
    "    # preprocess and tokenize input sentence\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    tokens = mecab.morphs(sentence)\n",
    "    tokenized_sentence = [word_to_index.get(token, word_to_index[\"<unk>\"]) for token in tokens]\n",
    "\n",
    "    # get predicted translation\n",
    "    translation = evaluate(sentence)\n",
    "    candidate = translation.split()\n",
    "\n",
    "    # get reference translation\n",
    "    reference = reference_sentence.split()\n",
    "\n",
    "    # calculate BLEU score\n",
    "    score = sentence_bleu([reference], candidate, smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Source Sentence: \", sentence)\n",
    "        print(\"Model Prediction: \", candidate)\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_question = \"이따 만나요.\"\n",
    "sample_answer = \"네 조금 이따 봐요.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  이따 만나요.\n",
      "Model Prediction:  ['이따', '마주치', '요', '는데', '<end>']\n",
      "Real:  ['네', '조금', '이따', '봐요.']\n",
      "Score: 0.053728\n",
      "\n",
      "Bleu Score for the single sentence pair: 0.05372849659117709\n"
     ]
    }
   ],
   "source": [
    "score = calculate_bleu_single(sample_question, sample_answer)\n",
    "print(f\"Bleu Score for the single sentence pair: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(sentences, reference_sentences, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(sentences)\n",
    "\n",
    "    for idx in range(sample_size):\n",
    "        score = calculate_bleu_single(sentences[idx], reference_sentences[idx], verbose)\n",
    "        total_score += score\n",
    "\n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_questions = questions[:10]  \n",
    "sample_answers = answers[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  12시 땡!\n",
      "Model Prediction:  ['하루', '가', '또', '가', '캐치', '<end>']\n",
      "Real:  ['하루가', '또', '가네요.']\n",
      "Score: 0.040825\n",
      "\n",
      "Source Sentence:  1지망 학교 떨어졌어\n",
      "Model Prediction:  ['위로', '해', '드립니다', '.', '<end>']\n",
      "Real:  ['위로해', '드립니다.']\n",
      "Score: 0.000000\n",
      "\n",
      "Source Sentence:  3박4일 놀러가고 싶다\n",
      "Model Prediction:  ['신혼', '여행', '4', '일', '놀', 'ㄹ래', '가', '고', '싶', '다', '<end>']\n",
      "Real:  ['여행은', '언제나', '좋죠.']\n",
      "Score: 0.000000\n",
      "\n",
      "Source Sentence:  3박4일 정도 놀러가고 싶다\n",
      "Model Prediction:  ['신혼', '여행', '4', '일', '정도', '놀', '러', '가', '고', '싶', '다', '<end>']\n",
      "Real:  ['여행은', '언제나', '좋죠.']\n",
      "Score: 0.000000\n",
      "\n",
      "Source Sentence:  ppl 심하네\n",
      "Model Prediction:  ['눈살', '이', '찌푸려', '지', '죠', '.', '<end>']\n",
      "Real:  ['눈살이', '찌푸려지죠.']\n",
      "Score: 0.000000\n",
      "\n",
      "Source Sentence:  sd카드 망가졌어\n",
      "Model Prediction:  ['다시', '새로', '사', '는', '게', '살', '다시', '다시', '다시', '조절', '<end>']\n",
      "Real:  ['다시', '새로', '사는', '게', '마음', '편해요.']\n",
      "Score: 0.044116\n",
      "\n",
      "Source Sentence:  sd카드 안돼\n",
      "Model Prediction:  ['다시', '새로', '안', '돼', '<end>']\n",
      "Real:  ['다시', '새로', '사는', '게', '마음', '편해요.']\n",
      "Score: 0.093026\n",
      "\n",
      "Source Sentence:  sns 맞팔 왜 안하지\n",
      "Model Prediction:  ['맞', '팔', '팔', '왜', '안', '하', '지', '꼼짝', '<end>']\n",
      "Real:  ['잘', '모르고', '있을', '수도', '있어요.']\n",
      "Score: 0.000000\n",
      "\n",
      "Source Sentence:  sns 시간낭비인 거 아는데 매일 하는 중\n",
      "Model Prediction:  ['시간', '을', '정하', '고', '새', '거', '아', '매일', '하', '는', '중', '<end>']\n",
      "Real:  ['시간을', '정하고', '해보세요.']\n",
      "Score: 0.000000\n",
      "\n",
      "Source Sentence:  sns 시간낭비인데 자꾸 보게됨\n",
      "Model Prediction:  ['시간', '을', '정하', '고', '인데', '보', '세요', '.', '<end>']\n",
      "Real:  ['시간을', '정하고', '해보세요.']\n",
      "Score: 0.000000\n",
      "\n",
      "Num of Sample: 10\n",
      "Total Score: 0.01779668963965857\n"
     ]
    }
   ],
   "source": [
    "calculate_bleu(sample_questions, sample_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SmoothingFunction()으로 BLEU Score 보정하기\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_single(sentence, reference_sentence, verbose=True, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    # preprocess and tokenize input sentence\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    tokens = mecab.morphs(sentence)\n",
    "    tokenized_sentence = [word_to_index.get(token, word_to_index[\"<unk>\"]) for token in tokens]\n",
    "\n",
    "    # get predicted translation\n",
    "    translation = evaluate(sentence)\n",
    "    candidate = translation.split()\n",
    "\n",
    "    # get reference translation\n",
    "    reference = reference_sentence.split()\n",
    "\n",
    "    # calculate BLEU score with smoothing function\n",
    "    score = sentence_bleu([reference], \n",
    "                          candidate, \n",
    "                          weights=weights, \n",
    "                          smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Source Sentence: \", sentence)\n",
    "        print(\"Model Prediction: \", candidate)\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_question = \"이따 만나요.\"\n",
    "sample_answer = \"네 조금 이따 봐요.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  이따 만나요.\n",
      "Model Prediction:  ['이따', '마주치', '요', '는데', '<end>']\n",
      "Real:  ['네', '조금', '이따', '봐요.']\n",
      "Score: 0.053728\n",
      "\n",
      "Bleu Score for the single sentence pair: 0.05372849659117709\n"
     ]
    }
   ],
   "source": [
    "score = calculate_bleu_single(sample_question, sample_answer)\n",
    "print(f\"Bleu Score for the single sentence pair: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # Apply smoothing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "reference_sentence = \"저는 책을 좋아합니다.\"\n",
    "predicted_sentence = \"저는 책을 좋아해요.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_tokens = reference_sentence.split()\n",
    "predicted_tokens = predicted_sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu Score: 0.24028114141347542\n"
     ]
    }
   ],
   "source": [
    "bleu_score = calculate_bleu(reference_tokens, predicted_tokens)\n",
    "print(f\"Bleu Score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **18-2. 프로젝트 제출**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **루브릭**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **평가문항 및 상세기준**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 챗봇 훈련데이터 전처리 과정이 체계적으로 진행되었는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "챗봇 훈련데이터를 위한 전처리와 augmentation이 적절히 수행되어 3만개 가량의 훈련데이터셋이 구축되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. transformer 모델을 활용한 챗봇 모델이 과적합을 피해 안정적으로 훈련되었는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과적합을 피할 수 있는 하이퍼파라미터 셋이 적절히 제시되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. 챗봇이 사용자의 질문에 그럴듯한 형태로 답하는 사례가 있는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 예문을 포함하여 챗봇에 던진 질문에 적절히 답하는 사례가 제출되었다."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
